{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4f14ba",
   "metadata": {},
   "source": [
    "# Lab Sheet 7 (COM3502-4502-6502 Speech Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c303656",
   "metadata": {},
   "source": [
    "This lab sheet is part of the lecture COM3502-4502-6502 Speech Processing at the [University of Sheffield](https://www.sheffield.ac.uk/ \"Open web page of The University of Sheffield\"), School of [Computer Science](https://www.sheffield.ac.uk/cs \"Open web page of School of Computer Science, University of Sheffield\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04248901",
   "metadata": {},
   "source": [
    "It is probably easiest to open this Jupyter Notebook with [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true \"Open in Google Colab\") since GitHub's Viewer does not always show all details correctly. <a href=\"https://colab.research.google.com/github/sap-shef/SpeechProcesssingLab/blob/main/Lab-Sheets/Lab-Sheet-8.ipynb\"><img align=\"right\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Notebook in Google Colab\" title=\"Open and Execute the Notebook directly in Google Colaboratory\"></a>\n",
    "\n",
    "Please put questions, comments and correction suggestions in the [Blackboard](https://vle.shef.ac.uk) discussion board or send an email to [s.goetze@sheffield.ac.uk](mailto:s.goetze@sheffield.ac.uk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9bb29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" id='ILOs'>\n",
    "<strong>Intended Learning Objectives (ILOs):</strong><br>\n",
    "    \n",
    "After completing this Jupyter Notebook you should\n",
    "    \n",
    "<ul>\n",
    "<li>have familiarised yourself with the concept of the <a href=\"https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method\"><code>Overlap-add algorithm</code></a>\n",
    "</li> \n",
    "<li>Be able to implement your own Overlap-add function\n",
    "</li>    \n",
    "<li>Understand the concept of <a href=\"https://wiki.aalto.fi/display/ITSP/Windowing\"><code>Perfect Reconstruction</code></a> and the Princen-Bradley criteria\n",
    "</li>\n",
    "<li>familiarise yourself with the basic use of the <code>Python</code> libraries \n",
    "    <ul>\n",
    "    <li><a href=\"https://numpy.org/doc/stable/index.html\"><code>Numpy</code></a> commands \n",
    "        <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.pad.html\"><code>pad()</code></a>,\n",
    "        <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.append.html\"><code>append()</code></a>,\n",
    "        <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.hamming.html\"><code>hamming()</code></a>,\n",
    "        <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.hanning.html\"><code>hanning()</code></a>,\n",
    "        <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.blackman.html\"><code>blackman()</code></a>,\n",
    "        etc.\n",
    "    </li>\n",
    "    <li><a href=\"https://matplotlib.org/\"><code>Matplotlib</code></a> for graphical output (like \n",
    "        <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.gca.html\"><code>gca()</code></a>, \n",
    "        <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.gcf.html\"><code>gcf()</code></a>, \n",
    "        etc.)\n",
    "    </li>\n",
    "    </ul>\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33f3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the ususal necessary and nice-to-have imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import seaborn as sns; sns.set() # styling ((un-)comment if you want)\n",
    "import numpy as np               # math\n",
    "\n",
    "# imports we need in addition for this lab sheet\n",
    "from IPython import display as ipd\n",
    "import scipy.signal as sig\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import read as wavread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c0f2e",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a id='task_1'></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task 1:**\n",
    "    \n",
    "<ul>\n",
    "<li> \n",
    "    Load a WAVE file containing speech, e.g. <code>speech_8kHz_murder.wav</code> from the Internet address <code>https://staffwww.dcs.shef.ac.uk/people/S.Goetze/sound/</code> and load it into a variable <code>s</code>.\n",
    "</li>\n",
    "    <li> \n",
    "    Generate a signal of white noise of the same length as your speech signal and load it into a variable <code>n</code>. Refer to <a href=\"https://colab.research.google.com/github/sap-shef/SpeechProcesssingLab/blob/main/Lab-Sheet-Solutions/Lab-Sheet-6-Solution.ipynb#scrollTo=18ff79b8\">Lab Sheet 6</a> for help. Normalise the amplitude of the noise signal to have an ampliude of $0.2$ (to avoid that it is too loud).\n",
    "</li>\n",
    "  <li>   \n",
    "    Create a microphone signal $y[k] = s[k]+n[k]$ as shown in the schematic below.\n",
    "\n",
    "<img id='FigNRSingleChannelBasic' src=\"https://staffwww.dcs.shef.ac.uk/people/S.Goetze/book/Chp5SignalEnhancementNR/images/NRSingleChannelBasic-web.png\" align=\"center\"/>\n",
    "      <center><span style=\"font-size:smaller\">\n",
    "    <b>Fig. 1:</b> Noise reduction filter $h[k]$ aiming to reduce the noise component $n[k]$ from the noisy microphone signal $y[k]$ to obtain an estimate of the clean speech signal $\\hat{s}[k]$.\n",
    "</span></center>\n",
    "    </li> \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here:\n",
    "# load speech wave into variable, s\n",
    "#...\n",
    "\n",
    "# generate white noise signal in variable, n\n",
    "#...\n",
    "\n",
    "# combine both signals to create your microphone signal, y\n",
    "#...\n",
    "\n",
    "# listen to your signal (if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755bb83",
   "metadata": {},
   "source": [
    "# Overlap-add Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a874385",
   "metadata": {},
   "source": [
    "When processing signals with long duration, it is common practise to do so by splitting the long signal into a series of shorter segments or chunks, a.k.a. frames. This is primarily for three reasons:\n",
    "\n",
    "<ul>\n",
    "<li> First, dealing with the full signal 'in one go' might be impractical due to constraints in computational memory (especially if you have to implement algorithms on ressource-limited hardware, such as r.g. microcontrollers). \n",
    "</li> \n",
    "<li> Second, to attain useful inference from operations such as the discrete Fourier Transform, it is assumed that the signal under scrutiny is time invariant (i.e its statistical properties do not vary with time). This is only typically true when analysing signals over short time frames. \n",
    "</li> \n",
    "<li> Third, for streaming applications involving block processing, such as a live spectrogram, adaptive noise reduction for a phone call, you may be interested in the result whilst still obtaining input data.\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "A simple algorithm to perform such chunk-wise processing is the [Overlap-Add method](https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method).\n",
    "\n",
    "The Overlap-Add method can be broken down into 3 stages:\n",
    "<ul>\n",
    "<li> \n",
    "    <strong>Segmentaion</strong> - Extract signal chunks (frames) of length $L$ by iteratively applying a windowing function, such as a Hanning window, to each chunk with each chunk overlapping by a number of $M$ samples. $M$ is often chosen to be $L/2$ in speech processing, i.e the overlapping fraction is that of the window length halved.\n",
    "</li> \n",
    "<li>\n",
    "    <strong>Processing</strong> - Each windowed chunk is processed independently, by convolving with a FIR filter for example.\n",
    "</li>\n",
    "<li>\n",
    "    <strong>Reconstruction</strong> - The processed chunks are 'added' back together sequentially to retain each chunk's relative position in the original signal.\n",
    "</li>\n",
    "</ul>\n",
    "    \n",
    "\n",
    "The process is described mathematically below where a long signal, $x[k]$, is to be convolved with a finite impulse response filter, $h[k]$.  \n",
    "\n",
    "1.&emsp;Applying a window function centered at the $i$th sample with window $w[k]$ of length $L$, whose value is 0 beyond $-L/2$ and $+L/2$:\n",
    "\n",
    "\\begin{equation}\n",
    "x_i[k] = x[k]w^L_i[k]\n",
    "\\label{eq:Windowing} \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "2.&emsp;Fragmentation of signal:\n",
    "\n",
    "\\begin{equation}\n",
    "x[k] = x_0[k] + x_1[k] + x_2[k] + ... \n",
    "\\label{eq:WindowedSegments} \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "<img id='FigBlockProcessing' src=\"https://staffwww.dcs.shef.ac.uk/people/S.Goetze/book/Chp2Fundamentals/images/signals-block-processing-web.png\" align=\"center\" width=\"700px\"/>\n",
    "      <center><span style=\"font-size:smaller\">\n",
    "    <b>Fig. 2:</b> Generating signals chunks / frames from longer signal.\n",
    "</span></center>\n",
    "\n",
    "[Fig. 2](#FigBlockProcessing) illustrates the split of the sequence $x[k]$ in the upper panel into chunks / frames $x_i[k]$. The windows $w_i[k]$ for each chunk $i$ are illustrated by dashed red lines. \n",
    "\n",
    "3.&emsp;Application of filter to fragmented signal: \n",
    "\n",
    "$$y[k]=\\sum_{i=0}^{N-1}x_{k-i}\\cdot h[i]$$  \n",
    "\n",
    "4.&emsp;Use the distributive property of convolution to formulate as blockwise:\n",
    "\\begin{align*}\n",
    "y[k]=&\\sum_{i=0}^{N-1}x[k-i]h[i]\n",
    "\\\\=&\\sum_{i=0}^{N-1}(x_{0}[k-i]+x_{1}[k-i]+...)\\cdot h[i]\n",
    "\\\\=&\\sum_{i=0}^{N-1}x_{0}[k-i]\\cdot h[i]+\\sum_{i=0}^{N-1}x_{1}[k-i]\\cdot h[i] + ...\n",
    "\\\\=&\\:\\mathrm{IFFT}\\left \\{ x_{0}[n]h[n]\\right \\} + \\mathrm{IFFT}\\left \\{ x_{1}[n]h[n]\\right \\} +...\n",
    "\\\\=&\\: y_{0}[k] + y_{1}[k] + ...\n",
    "\\end{align*}\n",
    "\n",
    "Since speech signals are time variant in nature, it is beneficial to utilise the Overlap-Add algorithm during the processing of them. This is based on the assumption that, heuristically speaking, fragmented speech present in each chunk may roughly represents a phoneme.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<a id='task_2'></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task 2: Perform windowing on your modelled microphone signal.**\n",
    "    \n",
    "<ul>\n",
    "<li> Extract a short temporal range from your noisy microphone signal $y[k]$; e.g. $\\approx 0.5$ second of audio.\n",
    "</li>     \n",
    "<li>\n",
    "    Finish the below function to iterate through said range applying a windowing function from the numpy library to each iteration. \n",
    "</li>\n",
    "<li>\n",
    "       Have the function return a list or np.array() all extracted chunks/frames, with each chunk padded with zeros so that all chunks are of the same length as the original input.\n",
    "</li>\n",
    "<li>\n",
    "       Test your function with the 'Blackman' filter.\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e6e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_chunks(sample, chunk_length = 0.03, fs = 8000, window_choice = 'Blackman'):\n",
    "    \"\"\"\n",
    "    Extracts windowed segments from a signal for a given chunk/frame length using a windowing\n",
    "    function from the numpy library.\n",
    "    \n",
    "    Input:\n",
    "        sample: list or np.array\n",
    "            input signal to be split into chunks/frames\n",
    "        chunk_length: float\n",
    "            desired window length in seconds\n",
    "        fs: int\n",
    "            sampling frequency of signal\n",
    "        window_choice: str \n",
    "            windowing function to use, one of: Blackman, Hamming, Hanning\n",
    "    Output:\n",
    "        list of np.arrays\n",
    "    \n",
    "    Example:\n",
    "       list_of_windows = get_windowed_chunks(sample, chunk_length = 0.025, sr=8000, window_choice = 'hanning')\n",
    "    \"\"\"\n",
    "    ## suggested method: \n",
    "    # find chunk centres first\n",
    "    # iterate through your window centres with a for loop\n",
    "    # use a dictionary to accept the window choice argument as an input and output the appropriate function from np\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada06c8",
   "metadata": {},
   "source": [
    "## Princen-Bradley Criteria\n",
    "\n",
    "A desired property of the windowing process used in an Overlap-Add algorithm is that the windowing itself should not significantly alter the original signal after signal reconstruction. This is known as the Princen-Bradley Criteria.\n",
    "\n",
    "\n",
    "<br>\n",
    "<a id='task_3'></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task 3: Princeton-Bradley Criteria**\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li> Try your windowing function with the chunk length of 0.03 seconds and window_choice = 'Blackman'. Do you think the output from this configuration satisfies the Princeton-Bradley Criteria? <br>\n",
    "    <strong>Hint</strong>: You can test this by simply summing the outputs (windowed segments) and by calculating the difference to the microphone signal before splitting into frames and windowing.\n",
    "</li>     \n",
    "<li>\n",
    "    Do the same using the Hanning window.\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8428e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# reconstruct your windowed signal\n",
    "#...\n",
    "\n",
    "# plot\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a2c3e",
   "metadata": {},
   "source": [
    "## Weiner Filtering\n",
    "A simple method to reduce the effects of the white noise we added to our modelled microphone signal in task 1 is to apply a Wiener filter. The Wiener filter is defined below where $h[n]$ is the impulse response of the filter $\\Phi_{yy}$, is the Power Spectral Density of the filter and $\\Phi_{n}$ is the Power Spectral Density of the filter.\n",
    "\n",
    "$$h[n]=\\frac{\\Phi_{yy}-\\Phi_{n}}{\\Phi_{yy}}$$\n",
    "\n",
    "<br>\n",
    "<a id='task_1'></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Task 4: Apply the Wiener filter to 2 chunks of your modelled microphone signal**\n",
    "    \n",
    "<ul>\n",
    "<li> Use scipy.sig.wiener() to apply Wiener filtering to each of the 2 chunks from your modelled microphone signal.\n",
    "</li>  \n",
    "<li> Plot the power spectral density function before and after applying the filter for each chunk.\n",
    "</li>     \n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93bca738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#your code here..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9da695",
   "metadata": {},
   "source": [
    "## Overlap-Add Function\n",
    "Now that we have observed the effects of the Wiener filter and how windowing can be performed, you should now have the tools to create your own Overlapp-add Weiner filtering function. \n",
    "\n",
    "<br>\n",
    "<a id='task_1'></a>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "    \n",
    "**Task 5: Finish the function below to implement the Overlap-Add algorithm to perform Weiner filtering.**\n",
    "    \n",
    "<ul>\n",
    "<li> Create a list of evenly spaced chunks from your signal, with a 50% overlapping fraction.\n",
    "</li>     \n",
    "<li> Apply a Hanning window to each chunk.\n",
    "</li>     \n",
    "<li> \n",
    "   Perform Wiener filtering using the scipy library on each chunk.\n",
    "</li> \n",
    "<li> \n",
    "   Reconstruct each filtered chunk to obtain the full filtered signal.\n",
    "</li>\n",
    "<li>\n",
    "     Aurally compare with the non-filtered signal, have you successfully denoised the signal?\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7ba18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_add_filtering(signal, chunk_length):\n",
    "    \"\"\"\n",
    "    Overlap-Add filtering.\n",
    "    \n",
    "    Input:\n",
    "        signal: np.array or list\n",
    "            input signal to process\n",
    "        chunk_length: int\n",
    "            size of \n",
    "    Output:\n",
    "        filtered signal\n",
    "    \n",
    "    Example:\n",
    "       overlap_add_filtering(signal, testfilt, 1000)\n",
    "    \"\"\"\n",
    "    bits_to_shift_by = (L_I-1).bit_length() # number of bits necessary to represent in binary\n",
    "    L_F = 2<<bits_to_shift_by # length of fourier transform window\n",
    "    L_W = L_F - L_I + 1 # length of signal to be filtered in single window overlap iteration\n",
    "    \n",
    "    filt_F = np.fft.rfft(filt, n=L_F) # filter in frequency domain\n",
    "    \n",
    "    \n",
    "    #YOUR WORK HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    return signal_f[:len(signal)] #truncate to length of original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call to test your function\n",
    "# ...\n",
    "\n",
    "# aurally compare with noisy signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85166cfb",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "This notebook is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) to be used during the lecture COM3502-4502-6502 Speech Processing at the [University of Sheffield](https://www.sheffield.ac.uk/ \"Open web page of The University of Sheffield\"), School of [Computer Science](https://www.sheffield.ac.uk/cs \"Open web page of School of Computer Science, University of Sheffield\"). You may download, [clone](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) or [fork](https://docs.github.com/en/get-started/quickstart/fork-a-repo) it to your computer or private [GitHub](https://github.com/) account. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

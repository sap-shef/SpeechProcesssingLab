{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc36c15d",
   "metadata": {},
   "source": [
    "# LPC Video creation\n",
    "\n",
    "This notebook is used to put all functionality to create a video of the LPC process. It is not necessary to look at this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634065f",
   "metadata": {},
   "source": [
    "## If you are running this Notebook on Google Colab\n",
    "\n",
    "If you are running this Notebook on Google Colab you should store the files from the ZIP file downloaded from Blackboard to your Googe Drive first and then you have to give this Notebook access to these files.\n",
    " \n",
    "If you are running the code locally (e.g. in Jupyter Notebook) you don't need to adapt (or execute) the following cell.\n",
    "\n",
    "The following code assumes that you stores the LPC older directly in your main Google Drive folder. Please adapt the pathes accordingly if you chose a different storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d26c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True if you intend to work on Google Drive and with Google Colab\n",
    "I_AM_USING_GOOGLE_DRIVE = False \n",
    "\n",
    "# mount your Google Drive to be accessible from Google Colab\n",
    "if I_AM_USING_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    !cp drive/MyDrive/LPC/nb_importer.py .\n",
    "    !cp drive/MyDrive/LPC/lpc_synthesis.ipynb ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb23f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have ffmpeg installed uncomment the next line\n",
    "#!pip install ffmpeg-python\n",
    "\n",
    "# ffmpeg for combining video and audio\n",
    "import ffmpeg\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# for writing wave files\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from IPython.display import Video, display\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "# Import from other notebook\n",
    "import nb_importer\n",
    "from lpc_synthesis import rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_plotting_setup(video, sample_rate_hz, signal, signal_reconstructed, half_window_len_samples, p):\n",
    "    if not video:\n",
    "        return\n",
    "\n",
    "    color_original = \"C0\"\n",
    "    color_transfer = \"C1\"\n",
    "    color_original_secondary = \"C2\"\n",
    "    color_synthesized = \"C4\"\n",
    "    color_synthesized_secondary = \"C6\"\n",
    "    color_highlight = \"C3\"\n",
    "\n",
    "    # Prepare the plotting, i.e., set all static elements\n",
    "    fig = plt.figure(figsize=(12, 7), constrained_layout=True)\n",
    "    fig.suptitle(\"Linear Predictive Coding\")\n",
    "\n",
    "    # Create grid for the different plots (2 lines, 9 columns)\n",
    "    gs = fig.add_gridspec(3, 8)\n",
    "\n",
    "    # Add top left plot for different spectrum information\n",
    "    ax_spectrum = fig.add_subplot(gs[0, :4])\n",
    "    # sampling theorem, maximal frequency is half the sampling frequency\n",
    "    ax_spectrum.set_xlim((0, sample_rate_hz // 2))\n",
    "    ax_spectrum.set_ylim((-75, 75))\n",
    "    (spectrum,) = ax_spectrum.plot(\n",
    "        [], [], color=color_original, label=\"Original Signal Spectrum\"\n",
    "    )\n",
    "    (transfer,) = ax_spectrum.plot(\n",
    "        [], [], color=color_transfer, linestyle=\"--\", label=\"Transfer Function\"\n",
    "    )\n",
    "    (error_spectrum,) = ax_spectrum.plot(\n",
    "        [], [], color=color_original_secondary, label=\"Error Spectrum\"\n",
    "    )\n",
    "\n",
    "    ax_spectrum.legend()\n",
    "    ax_spectrum.set_title(\n",
    "        \"Original Signal Spectrum vs. Transfer Function vs. Error Spectrum\"\n",
    "    )\n",
    "    ax_spectrum.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax_spectrum.set_ylabel(\"Energy (dB)\")\n",
    "\n",
    "    ax_spectrum2 = fig.add_subplot(gs[0, 4:])\n",
    "    ax_spectrum2.set_xlim((0, sample_rate_hz // 2))\n",
    "    ax_spectrum2.set_ylim((-75, 75))\n",
    "    (error_spectrum2,) = ax_spectrum2.plot(\n",
    "        [], [], color=color_synthesized_secondary, label=\"Excitation Signal Spectrum\"\n",
    "    )\n",
    "    (transfer2,) = ax_spectrum2.plot(\n",
    "        [], [], color=color_transfer, linestyle=\"--\", label=\"Transfer Function\"\n",
    "    )\n",
    "    (spectrum2,) = ax_spectrum2.plot(\n",
    "        [], [], color=color_synthesized, label=\"Synthesized Signal Spectrum\"\n",
    "    )\n",
    "\n",
    "    ax_spectrum2.legend()\n",
    "    ax_spectrum2.set_title(\n",
    "        \"Excitation Signal Spectrum vs. Transfer Function vs. Synthesized Signal Spectrum\"\n",
    "    )\n",
    "    ax_spectrum2.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax_spectrum2.set_ylabel(\"Energy (dB)\")\n",
    "\n",
    "    ax_bar = fig.add_subplot(gs[1, 0])\n",
    "    bar = ax_bar.bar([0], [0], color=color_synthesized_secondary)\n",
    "    ax_bar.set_ylim((80, 250))\n",
    "    ax_bar.tick_params(\"x\", bottom=False, labelbottom=False)\n",
    "\n",
    "    ax_bar.set_ylabel(\"Fundamental Frequency (Hz)\")\n",
    "\n",
    "    ax_synth = fig.add_subplot(gs[1, 1:5])\n",
    "    (plt_synth,) = ax_synth.plot([], [], color=color_synthesized_secondary)\n",
    "    ax_synth.set_xlim((0, 2 * half_window_len_samples))\n",
    "    signal_max = np.max(np.abs(signal))\n",
    "    ax_synth.set_ylim((-3277, 6554))\n",
    "    ax_synth.set_xlabel(\"Time (samples)\")\n",
    "    ax_synth.set_title(\"Synthesised Excitation Signal Waveform\")\n",
    "\n",
    "    ax_glottis = fig.add_subplot(gs[1, 5:])\n",
    "    plt_glottis_top = ax_glottis.stairs([], color=color_synthesized, linewidth=2)\n",
    "    plt_glottis_bot = ax_glottis.stairs([], color=color_synthesized, linewidth=2)\n",
    "    ax_glottis.set_xlim((-1, p))\n",
    "    ax_glottis.set_ylim((-1.05, 1.05))\n",
    "    ax_glottis.set_axis_off()\n",
    "\n",
    "    ax_glottis.set_title(\"Vocal Tract and Glottis\")\n",
    "\n",
    "    (plt_glottis_activity,) = ax_glottis.plot([0], [0], \"ko\", markersize=20)\n",
    "\n",
    "    ax_waveform = fig.add_subplot(gs[2, :4])\n",
    "    ax_waveform.set_xlim((0, len(signal)))\n",
    "    ax_waveform.set_ylim((-signal_max, signal_max))\n",
    "    (plt_waveform,) = ax_waveform.plot(\n",
    "        np.arange(len(signal)), signal, color=color_original\n",
    "    )\n",
    "    plt_progress = ax_waveform.axvspan(0, 0, color=color_highlight, alpha=0.2)\n",
    "\n",
    "    ax_waveform.set_title(\"Original Signal Waveform\")\n",
    "    ax_waveform.set_xlabel(\"Time (samples)\")\n",
    "\n",
    "    ax_waveform2 = fig.add_subplot(gs[2, 4:])\n",
    "    ax_waveform2.set_xlim((0, len(signal)))\n",
    "    ax_waveform2.set_ylim((-signal_max, signal_max))\n",
    "    (plt_waveform2,) = ax_waveform2.plot(\n",
    "        np.arange(len(signal)), signal_reconstructed, color=color_synthesized\n",
    "    )\n",
    "    plt_progress2 = ax_waveform2.axvspan(0, 0, color=color_highlight, alpha=0.2)\n",
    "\n",
    "    ax_waveform2.set_title(\"Synthesized Signal Waveform\")\n",
    "    ax_waveform2.set_xlabel(\"Time (samples)\")\n",
    "\n",
    "    return (\n",
    "        fig,\n",
    "        spectrum,\n",
    "        transfer,\n",
    "        error_spectrum,\n",
    "        spectrum2,\n",
    "        transfer2,\n",
    "        error_spectrum2,\n",
    "        bar[0],\n",
    "        plt_synth,\n",
    "        plt_glottis_top,\n",
    "        plt_glottis_bot,\n",
    "        plt_glottis_activity,\n",
    "        plt_waveform,\n",
    "        plt_progress,\n",
    "        plt_waveform2,\n",
    "        plt_progress2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae64691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_plotting_frame(\n",
    "    video,\n",
    "    plotting,\n",
    "    block,\n",
    "    sample_rate_hz,\n",
    "    residual_signal,\n",
    "    block_synthesized,\n",
    "    excitation_signal_synthesized,\n",
    "    excitaton_frequency_hz,\n",
    "    noise_excitation_percentage,\n",
    "    k,\n",
    "    signal_energy,\n",
    "    index,\n",
    "    half_window_len_samples,\n",
    "    signal_reconstructed,\n",
    "    p,\n",
    "    a,\n",
    "):\n",
    "    if not video:\n",
    "        return\n",
    "\n",
    "    (\n",
    "        _,\n",
    "        spectrum,\n",
    "        transfer,\n",
    "        error_spectrum,\n",
    "        spectrum2,\n",
    "        transfer2,\n",
    "        error_spectrum2,\n",
    "        bar,\n",
    "        plt_synth,\n",
    "        plt_glottis_top,\n",
    "        plt_glottis_bot,\n",
    "        plt_glottis_activity,\n",
    "        plt_waveform,\n",
    "        plt_progress,\n",
    "        plt_waveform2,\n",
    "        plt_progress2,\n",
    "    ) = plotting\n",
    "\n",
    "    # --------\n",
    "    # PLOTTING\n",
    "    # --------\n",
    "    N = len(block)\n",
    "    # Get smallest power of 2 at least as large as N\n",
    "    NFFT = 2 ** math.ceil(math.log2(N))\n",
    "    freq_vec = np.linspace(0, sample_rate_hz // 2, NFFT // 2)\n",
    "    X = np.fft.rfft(block, NFFT) / N\n",
    "    X = 2 * np.abs(X[: NFFT // 2])\n",
    "\n",
    "    # Prevent zeros from messing up logarithm\n",
    "    X[X < 1e-10] = 1e-10\n",
    "    spectrum.set_data(freq_vec, 20 * np.log10(X))\n",
    "\n",
    "    E = np.fft.rfft(residual_signal, NFFT) / N\n",
    "    E = 2 * np.abs(E[: NFFT // 2])\n",
    "\n",
    "    error_spectrum.set_data(freq_vec, 20 * np.log10(E))\n",
    "\n",
    "    # transfer function\n",
    "    dirac_impulse = np.zeros(N)\n",
    "    dirac_impulse[0] = np.sqrt(N) * rms(residual_signal)\n",
    "    h = lfilter(np.array([1], dtype=a.dtype), a, dirac_impulse)\n",
    "    H = np.fft.rfft(h, NFFT) / N\n",
    "    H = 2 * np.abs(H[: NFFT // 2])\n",
    "\n",
    "    transfer.set_data(freq_vec, 20 * np.log10(H))\n",
    "\n",
    "    # SPECTRUM 2\n",
    "    X = np.fft.rfft(block_synthesized, NFFT) / N\n",
    "    X = 2 * np.abs(X[: NFFT // 2])\n",
    "\n",
    "    # Prevent zeros from messing up logarithm\n",
    "    X[X < 1e-10] = 1e-10\n",
    "    spectrum2.set_data(freq_vec, 20 * np.log10(X))\n",
    "\n",
    "    E = np.fft.rfft(excitation_signal_synthesized, NFFT) / N\n",
    "    E = 2 * np.abs(E[: NFFT // 2])\n",
    "\n",
    "    error_spectrum2.set_data(freq_vec, 20 * np.log10(E))\n",
    "\n",
    "    # transfer function\n",
    "    transfer2.set_data(freq_vec, 20 * np.log10(H))\n",
    "\n",
    "    # set excitation frequency\n",
    "    bar.set_height(excitaton_frequency_hz)\n",
    "    if not np.isnan(noise_excitation_percentage):\n",
    "        bar.set_alpha(1 - noise_excitation_percentage)\n",
    "\n",
    "    plt_synth.set_data(\n",
    "        np.arange(len(excitation_signal_synthesized)), excitation_signal_synthesized\n",
    "    )\n",
    "\n",
    "    # The reflection coefficients k are used to simulate an acoustic tube\n",
    "    # Compute the impedance ratios ([Mak], formula (45)) between consecutive sections of acoustic tube\n",
    "    g = (1 + k) / (1 - k)\n",
    "    # \"In the case of an acoustic tube with p sections of equal thickness, the impedance ratios reduce to the inverse ratio of the consecutive cross-sectional areas.\" [Mak], S. 566\n",
    "    # => computes ratios between cross-sections\n",
    "    ratios = 1 / g\n",
    "    # D will contain the diameters of the P sections\n",
    "    D = np.ones(p)\n",
    "    for i in range(1, p):\n",
    "        # Compute area from ratio and previous area\n",
    "        D[i] = D[i - 1] * ratios[i - 1]\n",
    "    # Normalize\n",
    "    D = D / np.max(D)\n",
    "    # Obtain diameter from area (due to normalization we don't care about the $\\pi$ factor here)\n",
    "    D = np.sqrt(D)\n",
    "    plt_glottis_top.set_data(values=D, edges=np.arange(p + 1))\n",
    "    plt_glottis_bot.set_data(values=-D, edges=np.arange(p + 1))\n",
    "\n",
    "    glottis_acitvity = max(min(20 * rms(residual_signal) / signal_energy, 1), 0)\n",
    "    plt_glottis_activity.set_markerfacecolor(\n",
    "        (1 - glottis_acitvity, 1 - glottis_acitvity, 1 - glottis_acitvity)\n",
    "    )\n",
    "\n",
    "    # Indicator in the waveform which block we are processing\n",
    "    plt_progress.set_xy(\n",
    "        [\n",
    "            [index * half_window_len_samples, 0],\n",
    "            [index * half_window_len_samples, 1],\n",
    "            [(index + 2) * half_window_len_samples, 1],\n",
    "            [(index + 2) * half_window_len_samples, 0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Indicator in the waveform which block we are processing\n",
    "    plt_waveform2.set_data(np.arange(len(signal_reconstructed)), signal_reconstructed)\n",
    "    plt_progress2.set_xy(\n",
    "        [\n",
    "            [index * half_window_len_samples, 0],\n",
    "            [index * half_window_len_samples, 1],\n",
    "            [(index + 2) * half_window_len_samples, 1],\n",
    "            [(index + 2) * half_window_len_samples, 0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return a list of all modified plot parts\n",
    "    return (\n",
    "        spectrum,\n",
    "        transfer,\n",
    "        bar,\n",
    "        plt_synth,\n",
    "        plt_glottis_top,\n",
    "        plt_glottis_bot,\n",
    "        plt_glottis_activity,\n",
    "        plt_progress,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_plotting_finish(video, plotting, anim, signal, signal_reconstructed, sample_rate_hz):\n",
    "    # Write animation to disk\n",
    "    anim.save(\"lpc-result-video.mp4\")\n",
    "    \n",
    "    plt.close(plotting[0])\n",
    "    \n",
    "    # If we want a video we also need to write the sound file to disk\n",
    "    sf.write(\"lpc-result-audio.wav\", np.asarray(signal_reconstructed, dtype=signal.dtype), sample_rate_hz)\n",
    "    # Now use FFmpeg to combine the video (without sound) and the sound to a video with sound\n",
    "    video = ffmpeg.input(\"lpc-result-video.mp4\")\n",
    "    audio = ffmpeg.input(\"lpc-result-audio.wav\")\n",
    "    ffmpeg.concat(video, audio, v=1, a=1).output(\"lpc-result.mp4\").run(\n",
    "        overwrite_output=True, quiet=True\n",
    "    )\n",
    "\n",
    "    # Display the video inside the notebook\n",
    "    display(Video(\"lpc-result.mp4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

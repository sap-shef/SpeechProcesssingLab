{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Lab-Sheet 9 (COM3502-4502-6502 Speech Processing)\n",
    "\n",
    "This lab sheet is part of the lecture COM[3502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/level3/com3502.html \"Open web page for COM3502 module\")-[4502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/level4/com4502.html \"Open web page for COM4502 module\")-[6502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/msc/com6502.html \"Open web page for COM4502 module\") Speech Processing at the [University of Sheffield](https://www.sheffield.ac.uk/ \"Open web page of The University of Sheffield\"), Dept. of [Computer Science](https://www.sheffield.ac.uk/dcs \"Open web page of Department of Computer Science, University of Sheffield\").\n",
    "***\n",
    "\n",
    "Usually, it is easiest to open this Jupyter Notebook with [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true \"Open in Google Colab\") since GitHub's Viewer does not always show all details correctly. <a href=\"https://colab.research.google.com/github/sap-shef/SpeechProcesssingLab/blob/main/Lab-Sheets/LPC/lpc.ipynb\"><img align=\"right\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Notebook in Google Colab\" title=\"Open and Execute the Notebook directly in Google Colaboratory\"></a>\n",
    "\n",
    "This (set of) Notebooks partly imports other Notebooks (see more details below). Therefore, use of Google Colab may be more difficult this time. However, there will be hints in the Notebooks how to achieve this.\n",
    "\n",
    "***\n",
    "\n",
    "Please put questions, comments and correction suggestions in the [Blackboard](https://vle.shef.ac.uk) discussion board or send an email to [s.goetze@sheffield.ac.uk](mailto:s.goetze@sheffield.ac.uk).\n",
    "\n",
    "***\n",
    "\n",
    "## Linear Predictive Coding (LPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "\n",
    "This Notebook is the main Notebook. It introduces some background knowledge and conatins the high-level implementation. However, the other notebooks are intended to form the basis for understanding and should therefore be inspected first. The following figure shows how the Notebooks are connected.\n",
    "\n",
    "![LPC needs Fundamental Frequency, LPC Analysis, and LPC Synthesis](images/flow_lpc.svg)\n",
    "\n",
    "\n",
    "1. To analyse the prediction error sequence $e[k]$ we first need to know [how to correlate two signals](./correlation.ipynb) followed by [how to autocorrelate a signal](./autocorrelation.ipynb). This knowledge will then be used to [find the fundamental frequency of a signal](./fundamental_frequency.ipynb). To ensure this knowledge, Notebooks [`correlation.ipynb`](./LPC/correlation.ipynb),  [`autocorrelation.ipynb`](./LPC/autocorrelation.ipynb) and [`fundamental_frequency.ipynb`](./LPC/fundamental_frequency.ipynb)  repeat content already known from lab sheet [Lab-Sheet 3](Lab-Sheet-3.ipynb) and [Lab-Sheet 7](Lab-Sheet-7.ipynb), respectively.\n",
    "\n",
    "2. For the analysis we first look at [how to extract the Linear Prediction coefficients $a_i$](./lpc_analysis.ipynb).\n",
    "\n",
    "3. The [synthesis](./lpc_synthesis.ipynb) is performed in its own notebook as well.\n",
    "\n",
    "\n",
    "<br>\n",
    "<a id='hints'></a>\n",
    "<div style=\"border: 2px solid #999; padding: 10px; background: #eee;\">\n",
    "    \n",
    "**Hints:**\n",
    "    \n",
    "<ul>\n",
    "<li> \n",
    "    At the end of this notebook we will generate a video. This make take up to roughly 20 minutes. To have this video ready once you get back to this notebook, please execute all cells in this notebook.\n",
    "</li>     \n",
    "<li> \n",
    "    Before continuing with this notebook, make sure you have worked through the other notebooks in the given order.\n",
    "</li>   \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intended Learning Outcomes (ILOs) of this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this notebook, students should be able to\n",
    "\n",
    "- *describe* the source-filter model and *relate* Linear Predictive Coding to it,\n",
    "- *understand* how to *implement* a function which encodes and decodes a speech signal via Linear Predictive Coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Theory - The Source-Filter Model for Speech Synthesis\n",
    "\n",
    "As learned before human speech production can be seen as a two-step process, described by the *source-filter model* which is visualized below.\n",
    "The first step is the sound *source*, in human speech production the vocal tract, and the resulting sound then passes through an acoustic *filter*, the vocal tract filter.\n",
    "\n",
    "The sound source can either be a series of impulses (as produced by glottis) or a noise signal (as produced by the pharynx) or a combination of both.\n",
    "This source signal is then modified by a linear filter (in humans by the vocal tract) to produce the final sound.\n",
    "\n",
    "![The Source-Filter Model](images/source_filter_model.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the speech synthesis depicted above we need knowlegde about the following quantities:\n",
    "- Ratio of voiced vs. unvoiced speech $\\alpha$\n",
    "- Frequency of the impulse train $f_0$\n",
    "- Vocal tract filter coefficients $a_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Theory - Linear Prediction\n",
    "\n",
    "*Linear Prediction* follows the idea that the samples of a speech signal can be *predicted* from a *linear* combination of a finite number $P$ of its previous samples, i.e., the $k$th sample of a signal $s[k]$ can be *predicted* by\n",
    "\\begin{equation}\n",
    "\\hat{s}[k] = a_1 s[k-1] + a_2 s[k-2] + \\ldots + a_k s[k-p] = \\sum_{p=1}^P a_k s[k-p] \\tag{1}\n",
    "\\end{equation}\n",
    "and the prediction error $e[k]$ for each sample $k$ is obtained by \n",
    "\\begin{equation}\n",
    "e[k] = s[k] - \\hat{s}[k] = s[k] - \\sum_{p=1}^P a_p s[k-p]. \\tag{2}\n",
    "\\label{eq:PredictionError}\n",
    "\\end{equation}\n",
    "\n",
    "If we take the $Z$-transform on both sides of (\\ref{eq:PredictionError}), we obtain\n",
    "\\begin{equation}\n",
    "E[z] = \\left[1-\\sum_{p=1}^P a_pz^{-p}\\right]S[z] \\tag{3}\n",
    "\\end{equation}\n",
    "which then yields the filter transfer function\n",
    "\\begin{equation}\n",
    "H_1[z] = \\frac{E[z]}{S[z]} = 1-\\sum_{p=1}^P a_pz^{-p}, \\tag{4}\n",
    "\\end{equation}\n",
    "which is an *all-zero* filter, \n",
    "to obtain the prediction error sequence from the signal.\n",
    "\n",
    "Implementing the inverse filter to obtain the signal from the prediction error sequence gives us the *all-pole* filter\n",
    "\\begin{equation}\n",
    "H_2[z] = \\frac{S[z]}{E[z]} = \\frac{1}{1-\\sum_{p=1}^P a_pz^{-p}}. \\tag{5}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Theory - Calculating the Linear Predictive Coding (LPC) Parameters\n",
    "\n",
    "*Linear Predictive Coding (LPC)* now uses the coefficients obtained by Linear Prediction to encode a speech signal with less information with the ability to reconstruct it afterwards (implementing the source-filter model). This is done with the following steps:\n",
    "\n",
    "The *analysis / encoding* consists of the following steps:\n",
    "1. Extract Linear Prediction coefficients $a_p$.\n",
    "2. Obtain the residual signal $e[k]$ by filtering the signal $s[k]$ with the filter described by the $z$-domian transfer function $H_1[z]$, i.e., with the prediction coefficients as an *all-zero* filter.\n",
    "3. Analyse $e[k]$ to determine whether it is an impulsive signal (i.e. voiced excitation), a noise signal (i.e. voiceless excitation) or a mixture of both.\n",
    "    1. If it (partially) is an impulse signal, determine the frequency of the impulses $f_0$.\n",
    "    \n",
    "The *synthesis / decoding* consists of the following steps:\n",
    "1. Construct an excitation signal $e'[k]$ as a combination of\n",
    "    1. an impulse train signal with the previously determined impulse frequency $f_0$ and\n",
    "    2. a noise signal.\n",
    "2. Obtain a synthesized signal $s'[k]$ by filtering $e'[k]$ with the filter defined by the $z$-domain transfer function $H_2[z]$, i.e., with the prediction coefficients as an *all-pole* filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear Predictive Coding](images/lpc.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are running this Notebook on Google Colab\n",
    "\n",
    "If you are running this Notebook on Google Colab you should store the files from the ZIP file downloaded from Blackboard to your Googe Drive first and then you have to give this Notebook access to these files.\n",
    " \n",
    "If you are running the code locally (e.g. in Jupyter Notebook) you don't need to adapt (or execute) the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True if you intend to work on Google Drive and with Google Colab\n",
    "I_AM_USING_GOOGLE_DRIVE = False \n",
    "\n",
    "# mount your Google Drive to be accessible from Google Colab\n",
    "if I_AM_USING_GOOGLE_DRIVE:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code assumes that you stores the LPC older directly in your main Google Drive folder. Please adapt the pathes accordingly if you chose a different storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if I_AM_USING_GOOGLE_DRIVE:\n",
    "    !cp drive/MyDrive/LPC/nb_importer.py .\n",
    "    !cp drive/MyDrive/LPC/fundamental_frequency.ipynb .\n",
    "    !cp drive/MyDrive/LPC/lpc_analysis.ipynb .\n",
    "    !cp drive/MyDrive/LPC/lpc_synthesis.ipynb .\n",
    "    !cp drive/MyDrive/LPC/lpc_video.ipynb ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the other Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need some additional modules for plotting, working with audio files and displaying audio and video inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have ffmpeg installed uncomment the next line\n",
    "#!pip install ffmpeg-python\n",
    "#import ffmpeg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf  # for reading and writing wave files\n",
    "from IPython.display import HTML, Audio, Video, display  # to play audio\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module `nb_importer` allows us to import code from other notebooks as if they were regular Python files.\n",
    "We first import this module and then import the necessary functions from the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nb_importer\n",
    "\n",
    "from fundamental_frequency import fundamental_frequency\n",
    "from lpc_analysis import lpc_analysis\n",
    "from lpc_synthesis import excitation_signal, lpc_synthesis, rms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a function `lpc(signal, sample_rate_hz, windows_len_sec)` which performs LPC analysis and afterwards synthesis to reconstruct the original signal. The function will use weighted overlap-add to perform the analysis and synthesis on smaller blocks.\n",
    "\n",
    "The following steps will be performed in the code below:\n",
    "1. Calculate $P$, the number of Linear Prediction coefficients we will extract. A [useful approximation](https://wiki.aalto.fi/display/ITSP/Linear+prediction#Linearprediction-Physiologicalinterpretationandmodelorder) is to choose $P=\\text{round}\\left(1.25\\cdot\\frac{f_s}{1000}\\right)$, i.e., the sampling rate in kHz times $1.25$.\n",
    "2. Calculate the window size in samples and ensure it is divisible by $2$ so we can always have half windows overlap.\n",
    "3. Add zeros to the beginning and end of the signal s.t. there is half a window of zeros at the beginning and at least half a window of zeros at the end and that the number of samples is a multiple of half the window size.\n",
    "4. Create a `signal_reconstructed` with the same size of the padded signal, which will be filled with the reconstructed signal.\n",
    "5. We add some plotting preparation in case the user wants to generate a video of the LPC result.\n",
    "6. The inner `block_process` function is then called for each block (index) and actually performs the analysis and synthesis as follows:\n",
    "    1. Extract the block from the padded signal and apply a window function (the *weighted* part of weighted overlap-add).\n",
    "    2. Perform LPC analysis on the block to obtain $P$ prediction coefficients $a_p$.\n",
    "    3. Extract the frequency $f_0$ of the impulse part of the excitation signal and the estimated percentage of the noise part $\\alpha$ from the excitation signal (the filtered signal returned by the lpc analysis).\n",
    "    4. Convert the impulse frequency to number of samples and calculate the impulse offset based on the previous offset.\n",
    "    5. Synthesize a new signal block with the obtained parameters (prediction coefficients $a_p$, noise percentage $\\alpha$, impulse frequency $f_0$, ...).\n",
    "    6. Add a windowed version of the signal block onto the `signal_reconstructed`.\n",
    "7. Remove the padding from the reconstructed signal and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQDM (https://tqdm.github.io/) is a module used to show a progress bar for our video creation.\n",
    "# If the module is not installed we define our own dummy class which does nothing but provides\n",
    "# methods and properties which we will access in tqdm.\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    class tqdm:\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            self.n = 0\n",
    "            pass\n",
    "        def update(self, *args, **kwargs):\n",
    "            pass\n",
    "    \n",
    "from lpc_video import video_plotting_setup, video_plotting_frame, video_plotting_finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpc(signal, sample_rate_hz, window_len_sec, video=True):\n",
    "    number_of_samples = len(signal)\n",
    "    \n",
    "    # Estimate the number of Linear Prediction coefficients based on the sample rate\n",
    "    p = round(1.25 * sample_rate_hz / 1000)\n",
    "    \n",
    "    half_window_len_samples = round(window_len_sec * sample_rate_hz / 2)\n",
    "\n",
    "    # Optionally subtract the mean of the signal s.t. the signal has mean 0\n",
    "    # We convert the mean value to the same data type as the signal itself to be safe\n",
    "    signal -= np.mean(signal, dtype=signal.dtype)  # optional\n",
    "\n",
    "    signal_energy = rms(signal)\n",
    "\n",
    "    # Prepend and append half the window size many zeros.\n",
    "    # Also append zeros s.t. we have a multiple of the half window size.\n",
    "    signal = np.hstack(\n",
    "        (\n",
    "            np.zeros(half_window_len_samples, dtype=signal.dtype),\n",
    "            signal,\n",
    "            np.zeros(\n",
    "                2 * half_window_len_samples\n",
    "                - number_of_samples % half_window_len_samples,\n",
    "                dtype=signal.dtype,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    assert len(signal) % half_window_len_samples == 0\n",
    "\n",
    "    # For now the reconstructed signal size will be the same as the padded signal\n",
    "    signal_reconstructed = np.zeros(len(signal))\n",
    "\n",
    "    # Compute Hanning window\n",
    "    window = np.hanning(half_window_len_samples * 2)\n",
    "\n",
    "    # The number of windows we will look at (one less since we look at full not half windows)\n",
    "    number_of_windows = (len(signal) // half_window_len_samples) - 1\n",
    "\n",
    "    # Prepare the plotting of the video\n",
    "    plotting = video_plotting_setup(video, sample_rate_hz, signal, signal_reconstructed, half_window_len_samples, p)\n",
    "    \n",
    "    # Offset of the impuls excitation signal in the block\n",
    "    lpc.previous_impulse_offset = 0\n",
    "    lpc.previous_excitation_period = 0\n",
    "    lpc.impulse_offset = 0\n",
    "\n",
    "    # ----------------\n",
    "    # BLOCK PROCESSING\n",
    "    # ----------------\n",
    "    def block_process(index):\n",
    "        if index == t.n:\n",
    "            t.update()\n",
    "\n",
    "        block_start = index * half_window_len_samples\n",
    "        block_stop = (index + 2) * half_window_len_samples\n",
    "        # Extract the current block from the signal\n",
    "        block = signal[block_start:block_stop] * window\n",
    "\n",
    "        # Apply window function\n",
    "        block *= window\n",
    "\n",
    "        # Perform LPC analysis on the block\n",
    "        a, k, residual_signal = lpc_analysis(block, p)\n",
    "\n",
    "        # Obtain the fundamental frequency of the excitation signal.\n",
    "        excitaton_frequency_hz, noise_excitation_percentage = fundamental_frequency(\n",
    "            residual_signal, sample_rate_hz\n",
    "        )\n",
    "        # Convert it to the number of samples after which a pulse is repeated.\n",
    "        excitation_period = round(sample_rate_hz / excitaton_frequency_hz)\n",
    "\n",
    "        # Adjust the offset\n",
    "        # TODO: Does it make sense this way?\n",
    "        lpc.impulse_offset = (\n",
    "            lpc.impulse_offset - (half_window_len_samples % excitation_period)\n",
    "        ) % excitation_period\n",
    "\n",
    "        # Perform LPC synthesis and obtain synthesized block and excitation signal\n",
    "        block_synthesized, excitation_signal_synthesized = lpc_synthesis(\n",
    "            2 * half_window_len_samples,\n",
    "            a,\n",
    "            rms(residual_signal),\n",
    "            excitation_period,\n",
    "            lpc.impulse_offset,\n",
    "            noise_excitation_percentage,\n",
    "        )\n",
    "\n",
    "        # Window the synthesized block and add it to the synthesized signal\n",
    "        block_synthesized *= window\n",
    "        signal_reconstructed[block_start:block_stop] += block_synthesized\n",
    "\n",
    "        return video_plotting_frame(video, plotting, block, sample_rate_hz, residual_signal, block_synthesized, excitation_signal_synthesized, excitaton_frequency_hz, noise_excitation_percentage, k, signal_energy, index, half_window_len_samples, signal_reconstructed, p, a)\n",
    "\n",
    "    \n",
    "    # Initialize Progress bar\n",
    "    t = tqdm(total=number_of_windows, desc=\"Blocks\")\n",
    "    if video:\n",
    "        # If a video is requested we use the `block_process` function to not only perform the overlap-add but also\n",
    "        # render each frame of the video\n",
    "        anim = animation.FuncAnimation(\n",
    "            plotting[0],\n",
    "            block_process,\n",
    "            frames=number_of_windows,\n",
    "            interval=window_len_sec * 500,\n",
    "            blit=True,\n",
    "        )\n",
    "    else:\n",
    "        # If no video is requested, we just `block_process` each window\n",
    "        for index in range(number_of_windows):\n",
    "            block_process(index)\n",
    "\n",
    "    # Convert the signal to the same datatype as the original signal\n",
    "    video_plotting_finish(video, plotting, anim, signal, signal_reconstructed, sample_rate_hz)\n",
    "\n",
    "    # We want the signal without the additionally padded zeros\n",
    "    return np.asarray(signal_reconstructed[\n",
    "        half_window_len_samples : half_window_len_samples + number_of_samples\n",
    "    ], dtype=signal.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Download a WAVE file and load it to a variable `signal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "s_file_name = \"speech.wav\" # safe the downloaded file locally using this file name\n",
    "\n",
    "# download wave file\n",
    "if not os.path.exists(s_file_name):\n",
    "    !curl https://staffwww.dcs.shef.ac.uk/people/S.Goetze/sound/speech_8kHz_murder.wav -o {s_file_name}\n",
    "\n",
    "# load speech wave into variable\n",
    "signal, sample_rate_hz = sf.read(s_file_name, dtype=np.int16)\n",
    "# ensure that we only use one audio channel (mono)\n",
    "if len(signal.shape) == 2:\n",
    "    signal = signal[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display audio as playable and waveform plot\n",
    "display(Audio(signal, rate=sample_rate_hz))\n",
    "plt.plot(signal);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the main functions and will take some time to process (minutes), you will see a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform LPC, generate a video and return the reconstructed audio signal\n",
    "signal_reconstructed = lpc(\n",
    "    signal=signal, sample_rate_hz=sample_rate_hz, window_len_sec=0.02, video=True\n",
    ")\n",
    "display(Audio(signal_reconstructed, rate=sample_rate_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video shows many different things:\n",
    "\n",
    "1. The bottom plot shows the waveform of the original signal on the left and the reconstructed signal on the right. Both plots contain an indicator which window the current sound output and data in the other plots corresponds to.\n",
    "\n",
    "2. The plot on top shows on the left the spectrum of the current window of the original signal in blue, the transfer function in orange, and the spectrum of the error signal (after filtering) in green. Here we can clearly see that the filter *whitens* the signal, that is, it flattens the spectrum. On the right we see the spectrum of the synthesized excitation signal, the transfer function (same as on the left) and the spectrum of the resulting synthezied signal.\n",
    "\n",
    "3. The plots in the middle show the fundamental frequency extracted from the error signal (the more transparent it is the less impulse signal is contained in the excitation signal), the waveform of the synthesised excitation signal and a model of how the vocal tract would look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "This notebook is licensed to be used during the lecture COM[3502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/level3/com3502.html \"Open web page for COM3502 module\")-[4502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/level4/com4502.html \"Open web page for COM4502 module\")-[6502](http://www.dcs.shef.ac.uk/intranet/teaching/public/modules/msc/com6502.html \"Open web page for COM4502 module\") Speech Processing at the [University of Sheffield](https://www.sheffield.ac.uk/ \"Open web page of The University of Sheffield\"), Dept. of [Computer Science](https://www.sheffield.ac.uk/dcs \"Open web page of Department of Computer Science, University of Sheffield\"). Any further use (beyond use for the lecture) is only permitted if agreed with the [module lead](mailto:s.goetze@sheffield.ac.uk). \n",
    "### Contributors\n",
    "[Stefan Goetze](http://www.stefan-goetze.de \"Web page of Stefan Goetze\"),\n",
    "[Lena Strobl](https://github.com/sleyna/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Mak] Makhoul, J Linear Prediction: A tutorial review. Proc. IEEE. 63, 63, 56 (1975)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
